{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, mmengine, mmcv, mmdet, mmdet3d, spconv\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(mmengine.__version__)\n",
    "print(mmcv.__version__)\n",
    "print(mmdet.__version__)\n",
    "print(mmdet3d.__version__)\n",
    "print(spconv.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from mmengine.config import Config, DictAction\n",
    "from mmengine.logging import print_log\n",
    "from mmengine.registry import RUNNERS\n",
    "from mmengine.runner import Runner\n",
    "from mmengine import fileio\n",
    "\n",
    "from mmdet3d.utils import replace_ceph_backend\n",
    "from projects.mmdet3d_plugin.models.detectors import CmtDetector\n",
    "import time\n",
    "from mmengine.structures import InstanceData\n",
    "from mmdet.models.layers import inverse_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile('projects/configs/fusion/my_cmt_kitti.py')\n",
    "# cfg = Config.fromfile('../mmdetection3d/projects/BEVFusion/configs/my_bevfusion.py')\n",
    "cfg.work_dir = osp.abspath('./work_dirs')\n",
    "runner = Runner.from_cfg(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos2embed(pos, num_pos_feats=128, temperature=10000):\n",
    "    scale = 2 * math.pi\n",
    "    pos = pos * scale\n",
    "    dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=pos.device)\n",
    "    dim_t = 2 * (dim_t // 2) / num_pos_feats + 1\n",
    "    pos_x = pos[..., 0, None] / dim_t\n",
    "    pos_y = pos[..., 1, None] / dim_t\n",
    "    pos_x = torch.stack((pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()), dim=-1).flatten(-2)\n",
    "    pos_y = torch.stack((pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()), dim=-1).flatten(-2)\n",
    "    posemb = torch.cat((pos_y, pos_x), dim=-1)\n",
    "    return posemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    data_batch = next(iter(runner.train_dataloader))\n",
    "    data_batch = runner.model.data_preprocessor(data_batch, training=False)\n",
    "    batch_inputs_dict = data_batch['inputs']\n",
    "    batch_data_samples = data_batch['data_samples']\n",
    "    imgs = batch_inputs_dict.get('imgs', None)\n",
    "    points = batch_inputs_dict.get('points', None)\n",
    "    img_metas = [item.metainfo for item in batch_data_samples]\n",
    "    gt_bboxes_3d = [item.get('gt_instances_3d')['bboxes_3d'] for item in batch_data_samples]\n",
    "    gt_labels_3d = [item.get('gt_instances_3d')['labels_3d'] for item in batch_data_samples]\n",
    "\n",
    "    img_feats = runner.model.extract_img_feat(imgs, img_metas)\n",
    "    voxels, num_points, coors = runner.model.voxelize(points)\n",
    "    voxel_features = runner.model.pts_voxel_encoder(voxels, num_points, coors)\n",
    "    batch_size = coors[-1, 0] + 1\n",
    "    x1 = runner.model.pts_middle_encoder(voxel_features, coors, batch_size)\n",
    "    x2 = runner.model.pts_backbone(x1)\n",
    "    if runner.model.with_pts_neck:\n",
    "        pts_feats = runner.model.pts_neck(x2)\n",
    "\n",
    "    ret_dicts = []\n",
    "    x3 = runner.model.pts_bbox_head.shared_conv(pts_feats[0])\n",
    "    reference_points = runner.model.pts_bbox_head.reference_points.weight\n",
    "    reference_points, attn_mask, mask_dict = runner.model.pts_bbox_head.prepare_for_dn(x3.shape[0], reference_points, img_metas)\n",
    "    \n",
    "    rv_pos_embeds = runner.model.pts_bbox_head._rv_pe(img_feats[0], img_metas)\n",
    "    bev_pos_embeds = runner.model.pts_bbox_head.bev_embedding(pos2embed(runner.model.pts_bbox_head.coords_bev.to(x3.device), num_pos_feats=runner.model.pts_bbox_head.hidden_dim))\n",
    "    \n",
    "    bev_query_embeds, rv_query_embeds = runner.model.pts_bbox_head.query_embed(reference_points, img_metas)\n",
    "    query_embeds = bev_query_embeds + rv_query_embeds\n",
    "\n",
    "    outs_dec, _ = runner.model.pts_bbox_head.transformer(\n",
    "                        x3, img_feats[0], query_embeds,\n",
    "                        bev_pos_embeds, rv_pos_embeds,\n",
    "                        attn_masks=attn_mask\n",
    "                    )\n",
    "    outs_dec = torch.nan_to_num(outs_dec)\n",
    "    reference = inverse_sigmoid(reference_points.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 256, 100, 100]), torch.Size([4, 1130, 3]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.shape, reference_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 20, 60, 256]),\n",
       " torch.Size([10000, 256]),\n",
       " torch.Size([4, 1130, 256]),\n",
       " torch.Size([4, 1130, 256]),\n",
       " torch.Size([4, 1130, 256]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv_pos_embeds.shape, bev_pos_embeds.shape, bev_query_embeds.shape, rv_query_embeds.shape, query_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, torch.Size([4, 1130, 256]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outs_dec), outs_dec[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imgs[2][0].permute(1, 2, 0).cpu().numpy()\n",
    "img = mmcv.imdenormalize(img, mean=np.array([103.530, 116.280, 123.675]), std=np.array([57.375, 57.120, 58.395]), to_bgr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_bboxes_3d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feats[0].shape, img_feats[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_features.shape, x1.shape, x2[0].shape, x2[1].shape, x3[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程\n",
    "for data_batch in runner.train_dataloader:\n",
    "    data_batch = runner.model.data_preprocessor(data_batch, training=True)\n",
    "    if isinstance(data_batch, dict):\n",
    "        losses = runner.model(**data_batch, mode='loss')\n",
    "    elif isinstance(data_batch, (list, tuple)):\n",
    "        losses = runner.model(*data_batch, mode='loss')\n",
    "    else:\n",
    "        raise TypeError()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch['inputs'], data_batch['data_samples'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证过程\n",
    "for data_batch in runner.val_dataloader:\n",
    "    data_batch = runner.model.data_preprocessor(data_batch, training=False)\n",
    "    if isinstance(data_batch, dict):\n",
    "        outputs = runner.model(**data_batch, mode='predict')\n",
    "    elif isinstance(data_batch, (list, tuple)):\n",
    "        outputs = runner.model(**data_batch, mode='predict')\n",
    "    else:\n",
    "        raise TypeError()\n",
    "    runner.val_evaluator.process(data_samples=outputs, data_batch=data_batch)\n",
    "    break\n",
    "# with torch.no_grad():\n",
    "#     metrics = runner.val_evaluator.evaluate(len(runner.val_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch['inputs']['imgs'][0][:,300:400,300:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data_batch in runner.val_dataloader:\n",
    "        data_batch = runner.model.data_preprocessor(data_batch, training=False)\n",
    "        if isinstance(data_batch, dict):\n",
    "            outputs = runner.model(**data_batch, mode='predict')\n",
    "        elif isinstance(data_batch, (list, tuple)):\n",
    "            outputs = runner.model(**data_batch, mode='predict')\n",
    "        else:\n",
    "            raise TypeError()\n",
    "        num += 1\n",
    "        if num == 100:\n",
    "            break\n",
    "print(time.time() - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "with torch.no_grad():\n",
    "    data_batch_raw = next(iter(runner.val_dataloader))\n",
    "    for _ in range(100):\n",
    "        data_batch = runner.model.data_preprocessor(data_batch_raw, training=False)\n",
    "        if isinstance(data_batch, dict):\n",
    "            outputs = runner.model(**data_batch, mode='predict')\n",
    "        elif isinstance(data_batch, (list, tuple)):\n",
    "            outputs = runner.model(**data_batch, mode='predict')\n",
    "        else:\n",
    "            raise TypeError()\n",
    "print(time.time() - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    data_batch = next(iter(runner.val_dataloader))\n",
    "    data_batch = runner.model.data_preprocessor(data_batch, training=False)\n",
    "    batch_inputs_dict = data_batch['inputs']\n",
    "    batch_data_samples = data_batch['data_samples']\n",
    "    imgs = batch_inputs_dict.get('imgs', None)\n",
    "    points = batch_inputs_dict.get('points', None)\n",
    "    img_metas = [item.metainfo for item in batch_data_samples]\n",
    "\n",
    "    time_start = time.time()\n",
    "    for _ in range(100):\n",
    "        img_feats = runner.model.extract_img_feat(imgs, img_metas)\n",
    "    \n",
    "    midedle1 = time.time()\n",
    "    for _ in range(100):\n",
    "        pts_feats = runner.model.extract_pts_feat(points, img_feats, img_metas)\n",
    "\n",
    "    midedle2 = time.time()\n",
    "    for _ in range(100):\n",
    "        outs = runner.model.pts_bbox_head(pts_feats, img_feats, img_metas)\n",
    "\n",
    "    middle3 = time.time()\n",
    "    for _ in range(100):\n",
    "        bbox_list = runner.model.pts_bbox_head.get_bboxes(\n",
    "            outs, img_metas, rescale=False)\n",
    "        \n",
    "        # bbox_results = []\n",
    "        # for bboxes, scores, labels in bbox_list:\n",
    "        #     results = InstanceData()\n",
    "        #     results.bboxes_3d = bboxes.to('cpu')\n",
    "        #     results.scores_3d = scores.cpu()\n",
    "        #     results.labels_3d = labels.cpu()\n",
    "        #     bbox_results.append(results)\n",
    "        # detsamples = runner.model.add_pred_to_datasample(batch_data_samples,\n",
    "        #                                             data_instances_3d = bbox_results,\n",
    "        #                                             data_instances_2d = None)\n",
    "    end = time.time()\n",
    "    print(midedle1 - time_start, midedle2 - midedle1, middle3 - midedle2, end - middle3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "865d8b2eb28e274047ba64063dfb6a2aabf0dfec4905d304d7a76618dae6fdd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
